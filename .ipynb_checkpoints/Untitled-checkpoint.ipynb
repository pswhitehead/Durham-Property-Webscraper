{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import psycopg2\n",
    "from psycopg2 import sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertsql(command, data):\n",
    "    #Set up a connection to the PostgresSQL server we'll write data to\n",
    "    conn = psycopg2.connect(host=\"localhost\",database=\"durhamprop\", user=\"postgres\", password=\"postgres\")\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(command, data)\n",
    "    conn.commit()\n",
    "    \n",
    "    \n",
    "def getpropertydata(i):\n",
    "    \n",
    "    #Setp up the website we'll scrape data from\n",
    "    url = [\"https://property.spatialest.com/nc/durham/#/property/\"+str(i)]\n",
    "    \n",
    "    #Set up the Selenium web driver and then call the URL\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Firefox(options=options)\n",
    "    driver.get(url[0])\n",
    "    time.sleep(2) #Because we are loading the page, I wait a few seconds before the next command\n",
    "    \n",
    "    if driver.current_url == url[0]: #check the status isn't 404  or redirected before we scrape the info\n",
    "        #Parse the html\n",
    "        soup_ID = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        \n",
    "        #Find the key info\n",
    "        ki0 = re.findall(r'>(.*?)<', str(soup_ID.find(id = 'default_overview').findAll('span')[9]))[0] #landuse\n",
    "        ki1 = re.findall(r'>(.*?)<', str(soup_ID.find(id = 'default_overview').findAll('span')[13]))[0] #subdiv\n",
    "        ki2 = re.findall(r'>(.*?)<', str(soup_ID.find(id = 'default_overview').findAll('span')[21]))[0] #saledate\n",
    "        ki3 = re.findall(r'>(.*?)<', str(soup_ID.find(id = 'default_overview').findAll('span')[23]))[0] #saleprice\n",
    "        \n",
    "        #Find the buildinfo\n",
    "        bi0 = re.findall(r'>(.*?)<', str(soup_ID.find(id = 'default_buildings').findAll('span')[-1]))[0] #value\n",
    "        bi1 = re.findall(r'>(.*?)<', str(soup_ID.find(id = 'default_buildings').findAll('span')[4]))[0] #year built\n",
    "        bi2 = re.findall(r'>(.*?)<', str(soup_ID.find(id = 'default_buildings').findAll('span')[13]))[0] #area\n",
    "        bi3 = re.findall(r'>(.*?)<', str(soup_ID.find(id = 'default_buildings').findAll('span')[15]))[0] #bathrooms\n",
    "        bi4 = re.findall(r'>(.*?)<', str(soup_ID.find(id = 'default_buildings').findAll('span')[19]))[0] #bedrooms\n",
    "        bi5 = re.findall(r'>(.*?)<', str(soup_ID.find(id = 'default_buildings').findAll('span')[8]))[0] #use\n",
    "        bi6 = re.findall(r'>(.*?)<', str(soup_ID.find(id = 'default_buildings').findAll('span')[6]))[0] #builduse\n",
    "\n",
    "        #Find the assessment details\n",
    "        ad0 = re.findall(r'>(.*?)<', str(soup_ID.find(id = 'LandDetails').findAll('td')[0]))[0] #fair market value\n",
    "        ad1 = re.findall(r'>(.*?)<', str(soup_ID.find(id = 'LandDetails').findAll('td')[1]))[0] #land assessed value\n",
    "        ad2 = re.findall(r'>(.*?)<', str(soup_ID.find(id = 'LandDetails').findAll('td')[2]))[0] #acres\n",
    "        \n",
    "        #write sql code to insert\n",
    "        sqltemp_ki = \"\"\"INSERT INTO keyinfo(landuse,subdiv,saledate,saleprice,id)\n",
    "             VALUES(%s,%s,%s,%s,%s);\"\"\"\n",
    "        \n",
    "        sqltemp_bi = \"\"\"INSERT INTO buildinfo(value,year,area,bathrooms,bedrooms,use,builduse,id)\n",
    "             VALUES(%s,%s,%s,%s,%s,%s,%s,%s);\"\"\"\n",
    "        \n",
    "        sqltemp_ad = \"\"\"INSERT INTO assessment(fmv,lav,acres,id)\n",
    "             VALUES(%s,%s,%s,%s);\"\"\"\n",
    "        \n",
    "        #insert data in to sql\n",
    "        insertsql(sqltemp_ki, [ki0,ki1,ki2,ki3,i])\n",
    "        insertsql(sqltemp_bi, [bi0,bi1,bi2,bi3,bi4,bi5,bi6,i])\n",
    "        insertsql(sqltemp_ad, [ad0,ad1,ad2,i])\n",
    "    \n",
    "    driver.quit()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findsql():\n",
    "    #We are going to use this to query IDs that havent been scraped from SQL\n",
    "    \n",
    "    #Set up a connection to the PostgresSQL server we'll write data to\n",
    "    conn = psycopg2.connect(host=\"localhost\",database=\"durhamprop\", user=\"postgres\", password=\"postgres\")\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    #Write an sql query\n",
    "    query = sql.SQL(\"SELECT * FROM {table} WHERE {key} = 2 LIMIT 1 \").format(\n",
    "        table=sql.Identifier('scrapeprog'),\n",
    "        key=sql.Identifier('status'))\n",
    "    \n",
    "    #execute query\n",
    "    cur.execute(query)\n",
    "    return cur.fetchall()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                   \n",
    "        #get sale data if there\n",
    "        if soup_ID.find(id = 'Sales'):\n",
    "            for row in soup_ID.find(id = 'Sales').findAll('td'):\n",
    "                print(re.findall(r'>(.*?)<', str(row)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findsql()[0][1] == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Pool(10)  # Pool tells how many at a time\n",
    "records = p.map(parse, cars_links)\n",
    "p.terminate()\n",
    "p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = findsql()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
